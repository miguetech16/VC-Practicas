{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inicializa MediaPipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Configura la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Verifica si la cámara está disponible\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se puede acceder a la cámara.\")\n",
    "    exit()\n",
    "\n",
    "# Inicia el modelo de detección de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"No se pudo leer el cuadro de la cámara.\")\n",
    "            break\n",
    "\n",
    "        \n",
    "        # Convierte la imagen de BGR a RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Procesa la imagen para detectar poses\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Dibuja las anotaciones de pose en la imagen original\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Muestra la imagen procesada\n",
    "        cv2.imshow('Skeleton Detection', frame)\n",
    "\n",
    "        # Salir con la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configura la cámara y el video\n",
    "cap = cv2.VideoCapture(0)  # Cámara en vivo\n",
    "video_cap = cv2.VideoCapture('dance_video.mp4')  # Video del archivo\n",
    "\n",
    "# Variable para medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Inicia el modelo de detección de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        # Captura cuadro del video\n",
    "        ret_vid, frame_vid = video_cap.read()\n",
    "        if not ret_vid:\n",
    "            print(\"Se alcanzó el final del video.\")\n",
    "            video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia el video\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "\n",
    "        frame_vid = cv2.resize(frame_vid, None, fx=0.5, fy=0.5)\n",
    "        # Procesa el video\n",
    "        rgb_frame_vid = cv2.cvtColor(frame_vid, cv2.COLOR_BGR2RGB)\n",
    "        results_vid = pose.process(rgb_frame_vid)\n",
    "\n",
    "        if results_vid.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame_vid, results_vid.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Captura los puntos cada 30 segundos\n",
    "        current_time = time.time()\n",
    "        if current_time - start_time >= 30:\n",
    "\n",
    "            # Imprime los puntos del video en la terminal\n",
    "            if results_vid.pose_landmarks:\n",
    "                print(\"Puntos detectados (Video):\")\n",
    "                for id, landmark in enumerate(results_vid.pose_landmarks.landmark):\n",
    "                    print(f\"Punto {id}: (x: {landmark.x}, y: {landmark.y}, z: {landmark.z}, visibilidad: {landmark.visibility})\")\n",
    "                print(\"\\n---\\n\")\n",
    "\n",
    "            # Reinicia el contador de tiempo\n",
    "            start_time = current_time\n",
    "\n",
    "        # Muestra las imágenes procesadas\n",
    "        cv2.imshow('Skeleton Detection (Video)', frame_vid)\n",
    "\n",
    "        # Termina el bucle si se presiona la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuración del video de referencia\n",
    "dance_video = 'dance_video.mp4'\n",
    "\n",
    "# Función para extraer puntos de referencia y guardarlos en un archivo JSON\n",
    "def extraer_y_guardar_puntos(video_path, output_file):\n",
    "    puntos_video_referencia = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        video_cap = cv2.VideoCapture(video_path)\n",
    "        while True:\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "            if not ret_vid:\n",
    "                break\n",
    "\n",
    "            rgb_frame_vid = cv2.cvtColor(frame_vid, cv2.COLOR_BGR2RGB)\n",
    "            results_vid = pose.process(rgb_frame_vid)\n",
    "\n",
    "            if results_vid.pose_landmarks:\n",
    "                puntos = []\n",
    "                for landmark in results_vid.pose_landmarks.landmark:\n",
    "                    puntos.append({\n",
    "                        \"x\": landmark.x,\n",
    "                        \"y\": landmark.y,\n",
    "                        \"z\": landmark.z,\n",
    "                        \"visibility\": landmark.visibility\n",
    "                    })\n",
    "                puntos_video_referencia.append(puntos)\n",
    "\n",
    "        video_cap.release()\n",
    "\n",
    "    # Guarda los puntos en un archivo JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(puntos_video_referencia, f)\n",
    "\n",
    "    print(f\"Puntos de referencia guardados en {output_file}\")\n",
    "\n",
    "# Función para calcular la distancia relativa entre puntos clave\n",
    "def calcular_distancia_escala(puntos):\n",
    "    \"\"\"\n",
    "    Calcula una medida de escala basada en la distancia entre los hombros\n",
    "    (puntos 11 y 12 en Mediapipe Pose).\n",
    "    \"\"\"\n",
    "    hombro_izquierdo = puntos[11]\n",
    "    hombro_derecho = puntos[12]\n",
    "    return np.linalg.norm(np.array([hombro_izquierdo[\"x\"], hombro_izquierdo[\"y\"]]) -\n",
    "                          np.array([hombro_derecho[\"x\"], hombro_derecho[\"y\"]]))\n",
    "\n",
    "# Código para jugar comparando los puntos guardados\n",
    "\n",
    "def jugar_con_puntos(json_file, video_path):\n",
    "    # Carga los puntos del archivo JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        puntos_video_referencia = json.load(f)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Cámara en vivo\n",
    "    video_cap = cv2.VideoCapture(video_path)  # Video de referencia\n",
    "\n",
    "    def calcular_similitud(puntos1, puntos2, escala1, escala2):\n",
    "        \"\"\"\n",
    "        Calcula la similitud entre dos poses ajustando las coordenadas por sus escalas relativas.\n",
    "        \"\"\"\n",
    "        if not puntos1 or not puntos2 or len(puntos1) != len(puntos2):\n",
    "            return 0\n",
    "\n",
    "        distancias = []\n",
    "        for p1, p2 in zip(puntos1, puntos2):\n",
    "            p1_escalado = np.array([p1[\"x\"] / escala1, p1[\"y\"] / escala1, p1[\"z\"] / escala1])\n",
    "            p2_escalado = np.array([p2[\"x\"] / escala2, p2[\"y\"] / escala2, p2[\"z\"] / escala2])\n",
    "            distancias.append(np.linalg.norm(p1_escalado - p2_escalado))\n",
    "\n",
    "        return np.mean(distancias)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        # Leer cuadro del video de referencia\n",
    "        ret_vid, frame_vid = video_cap.read()\n",
    "        if not ret_vid:\n",
    "            video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia el video\n",
    "            frame_idx = 0\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "\n",
    "        if frame_idx < len(puntos_video_referencia):\n",
    "            puntos_referencia = puntos_video_referencia[frame_idx]\n",
    "\n",
    "            # Dibujar puntos y conexiones en el video de referencia\n",
    "            altura, ancho, _ = frame_vid.shape\n",
    "            for conexion in mp_pose.POSE_CONNECTIONS:\n",
    "                inicio = puntos_referencia[conexion[0]]\n",
    "                fin = puntos_referencia[conexion[1]]\n",
    "                inicio_px = (int(inicio[\"x\"] * ancho), int(inicio[\"y\"] * altura))\n",
    "                fin_px = (int(fin[\"x\"] * ancho), int(fin[\"y\"] * altura))\n",
    "                cv2.line(frame_vid, inicio_px, fin_px, (255, 0, 0), 2)\n",
    "\n",
    "            for punto in puntos_referencia:\n",
    "                px = (int(punto[\"x\"] * ancho), int(punto[\"y\"] * altura))\n",
    "                cv2.circle(frame_vid, px, 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Leer cuadro de la cámara en vivo\n",
    "        ret_cam, frame_cam = cap.read()\n",
    "        if not ret_cam:\n",
    "            print(\"No se puede acceder a la cámara.\")\n",
    "            break\n",
    "        \n",
    "        frame_cam = cv2.resize(frame_cam, None, fx=0.5, fy=0.5)\n",
    "        rgb_frame_cam = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            results_cam = pose.process(rgb_frame_cam)\n",
    "\n",
    "            if results_cam.pose_landmarks:\n",
    "                puntos_usuario = [\n",
    "                    {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z, \"visibility\": lm.visibility}\n",
    "                    for lm in results_cam.pose_landmarks.landmark\n",
    "                ]\n",
    "\n",
    "                if frame_idx < len(puntos_video_referencia):\n",
    "                    escala_referencia = calcular_distancia_escala(puntos_referencia)\n",
    "                    escala_usuario = calcular_distancia_escala(puntos_usuario)\n",
    "\n",
    "                    similitud = calcular_similitud(puntos_referencia, puntos_usuario, escala_referencia, escala_usuario)\n",
    "\n",
    "                    if similitud < 0.1:\n",
    "                        puntuacion = 100\n",
    "                    elif similitud < 0.2:\n",
    "                        puntuacion = 50\n",
    "                    else:\n",
    "                        puntuacion = 0\n",
    "\n",
    "                    cv2.putText(frame_cam, f'Puntuacion: {puntuacion}', (10, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            if results_cam.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame_cam, results_cam.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        # Mostrar ambos cuadros\n",
    "        combinado = np.hstack((cv2.resize(frame_vid, (640, 480)), cv2.resize(frame_cam, (640, 480))))\n",
    "        cv2.imshow('Comparacion: Video vs Usuario', combinado)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    video_cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    modo = input(\"Seleccione modo: 'extraer' para guardar puntos o 'jugar' para comparar: \")\n",
    "\n",
    "    if modo == 'extraer':\n",
    "        extraer_y_guardar_puntos(dance_video, 'puntos_referencia.json')\n",
    "    elif modo == 'jugar':\n",
    "        jugar_con_puntos('puntos_referencia.json', dance_video)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_PF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
