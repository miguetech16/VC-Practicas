{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# Inicializa MediaPipe\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_pose = mp.solutions.pose\n",
    "\n",
    "# Configura la cámara\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Verifica si la cámara está disponible\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: No se puede acceder a la cámara.\")\n",
    "    exit()\n",
    "\n",
    "# Inicia el modelo de detección de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"No se pudo leer el cuadro de la cámara.\")\n",
    "            break\n",
    "\n",
    "        \n",
    "        # Convierte la imagen de BGR a RGB\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Procesa la imagen para detectar poses\n",
    "        results = pose.process(rgb_frame)\n",
    "\n",
    "        # Dibuja las anotaciones de pose en la imagen original\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Muestra la imagen procesada\n",
    "        cv2.imshow('Skeleton Detection', frame)\n",
    "\n",
    "        # Salir con la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configura la cámara y el video\n",
    "cap = cv2.VideoCapture(0)  # Cámara en vivo\n",
    "video_cap = cv2.VideoCapture('dance_video.mp4')  # Video del archivo\n",
    "\n",
    "# Variable para medir el tiempo\n",
    "start_time = time.time()\n",
    "\n",
    "# Inicia el modelo de detección de pose\n",
    "with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "    while True:\n",
    "        # Captura cuadro del video\n",
    "        ret_vid, frame_vid = video_cap.read()\n",
    "        if not ret_vid:\n",
    "            print(\"Se alcanzó el final del video.\")\n",
    "            video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia el video\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "\n",
    "        frame_vid = cv2.resize(frame_vid, None, fx=0.5, fy=0.5)\n",
    "        # Procesa el video\n",
    "        rgb_frame_vid = cv2.cvtColor(frame_vid, cv2.COLOR_BGR2RGB)\n",
    "        results_vid = pose.process(rgb_frame_vid)\n",
    "\n",
    "        if results_vid.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame_vid, results_vid.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                mp_drawing.DrawingSpec(color=(255, 0, 0), thickness=2, circle_radius=2),\n",
    "                mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2)\n",
    "            )\n",
    "\n",
    "        # Captura los puntos cada 30 segundos\n",
    "        current_time = time.time()\n",
    "        if current_time - start_time >= 30:\n",
    "\n",
    "            # Imprime los puntos del video en la terminal\n",
    "            if results_vid.pose_landmarks:\n",
    "                print(\"Puntos detectados (Video):\")\n",
    "                for id, landmark in enumerate(results_vid.pose_landmarks.landmark):\n",
    "                    print(f\"Punto {id}: (x: {landmark.x}, y: {landmark.y}, z: {landmark.z}, visibilidad: {landmark.visibility})\")\n",
    "                print(\"\\n---\\n\")\n",
    "\n",
    "            # Reinicia el contador de tiempo\n",
    "            start_time = current_time\n",
    "\n",
    "        # Muestra las imágenes procesadas\n",
    "        cv2.imshow('Skeleton Detection (Video)', frame_vid)\n",
    "\n",
    "        # Termina el bucle si se presiona la tecla 'q'\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "# Libera los recursos\n",
    "cap.release()\n",
    "video_cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuración del video de referencia\n",
    "dance_video = 'dance_video.mp4'\n",
    "\n",
    "# Función para extraer puntos de referencia y guardarlos en un archivo JSON\n",
    "def extraer_y_guardar_puntos(video_path, output_file):\n",
    "    puntos_video_referencia = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        video_cap = cv2.VideoCapture(video_path)\n",
    "        while True:\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "            if not ret_vid:\n",
    "                break\n",
    "\n",
    "            rgb_frame_vid = cv2.cvtColor(frame_vid, cv2.COLOR_BGR2RGB)\n",
    "            results_vid = pose.process(rgb_frame_vid)\n",
    "\n",
    "            if results_vid.pose_landmarks:\n",
    "                puntos = []\n",
    "                for landmark in results_vid.pose_landmarks.landmark:\n",
    "                    puntos.append({\n",
    "                        \"x\": landmark.x,\n",
    "                        \"y\": landmark.y,\n",
    "                        \"z\": landmark.z,\n",
    "                        \"visibility\": landmark.visibility\n",
    "                    })\n",
    "                puntos_video_referencia.append(puntos)\n",
    "\n",
    "        video_cap.release()\n",
    "\n",
    "    # Guarda los puntos en un archivo JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(puntos_video_referencia, f)\n",
    "\n",
    "    print(f\"Puntos de referencia guardados en {output_file}\")\n",
    "\n",
    "# Función para calcular la distancia relativa entre puntos clave\n",
    "def calcular_distancia_escala(puntos):\n",
    "    \"\"\"\n",
    "    Calcula una medida de escala basada en la distancia entre los hombros\n",
    "    (puntos 11 y 12 en Mediapipe Pose).\n",
    "    \"\"\"\n",
    "    hombro_izquierdo = puntos[11]\n",
    "    hombro_derecho = puntos[12]\n",
    "    return np.linalg.norm(np.array([hombro_izquierdo[\"x\"], hombro_izquierdo[\"y\"]]) -\n",
    "                          np.array([hombro_derecho[\"x\"], hombro_derecho[\"y\"]]))\n",
    "\n",
    "# Código para jugar comparando los puntos guardados\n",
    "def jugar_con_puntos(json_file, video_path):\n",
    "    # Carga los puntos del archivo JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        puntos_video_referencia = json.load(f)\n",
    "\n",
    "    cap = cv2.VideoCapture(0)  # Cámara en vivo\n",
    "    video_cap = cv2.VideoCapture(video_path)  # Video de referencia\n",
    "\n",
    "    # Obtén la tasa de fotogramas por segundo (FPS) del video de referencia\n",
    "    fps_video = video_cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_duration = 1 / fps_video  # Duración de cada cuadro en segundos\n",
    "\n",
    "    def calcular_similitud(puntos1, puntos2, escala1, escala2):\n",
    "        \"\"\"\n",
    "        Calcula la similitud entre dos poses ajustando las coordenadas por sus escalas relativas.\n",
    "        \"\"\"\n",
    "        if not puntos1 or not puntos2 or len(puntos1) != len(puntos2):\n",
    "            return 0\n",
    "\n",
    "        distancias = []\n",
    "        for p1, p2 in zip(puntos1, puntos2):\n",
    "            p1_escalado = np.array([p1[\"x\"] / escala1, p1[\"y\"] / escala1, p1[\"z\"] / escala1])\n",
    "            p2_escalado = np.array([p2[\"x\"] / escala2, p2[\"y\"] / escala2, p2[\"z\"] / escala2])\n",
    "            distancias.append(np.linalg.norm(p1_escalado - p2_escalado))\n",
    "\n",
    "        return np.mean(distancias)\n",
    "\n",
    "    current_time = 0\n",
    "    while True:\n",
    "        # Leer cuadro del video de referencia\n",
    "        ret_vid, frame_vid = video_cap.read()\n",
    "        if not ret_vid:\n",
    "            video_cap.set(cv2.CAP_PROP_POS_FRAMES, 0)  # Reinicia el video\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "            current_time = 0\n",
    "\n",
    "        # Calcular el índice del JSON basado en el tiempo transcurrido\n",
    "        frame_idx = int(current_time * fps_video)\n",
    "        if frame_idx < len(puntos_video_referencia):\n",
    "            puntos_referencia = puntos_video_referencia[frame_idx]\n",
    "\n",
    "            # Dibujar puntos y conexiones en el video de referencia\n",
    "            altura, ancho, _ = frame_vid.shape\n",
    "            for conexion in mp_pose.POSE_CONNECTIONS:\n",
    "                inicio = puntos_referencia[conexion[0]]\n",
    "                fin = puntos_referencia[conexion[1]]\n",
    "                inicio_px = (int(inicio[\"x\"] * ancho), int(inicio[\"y\"] * altura))\n",
    "                fin_px = (int(fin[\"x\"] * ancho), int(fin[\"y\"] * altura))\n",
    "                cv2.line(frame_vid, inicio_px, fin_px, (255, 0, 0), 2)\n",
    "\n",
    "            for punto in puntos_referencia:\n",
    "                px = (int(punto[\"x\"] * ancho), int(punto[\"y\"] * altura))\n",
    "                cv2.circle(frame_vid, px, 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Leer cuadro de la cámara en vivo\n",
    "        ret_cam, frame_cam = cap.read()\n",
    "        if not ret_cam:\n",
    "            print(\"No se puede acceder a la cámara.\")\n",
    "            break\n",
    "\n",
    "        rgb_frame_cam = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)\n",
    "        with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "            results_cam = pose.process(rgb_frame_cam)\n",
    "\n",
    "            if results_cam.pose_landmarks:\n",
    "                puntos_usuario = [\n",
    "                    {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z, \"visibility\": lm.visibility}\n",
    "                    for lm in results_cam.pose_landmarks.landmark\n",
    "                ]\n",
    "\n",
    "                if frame_idx < len(puntos_video_referencia):\n",
    "                    escala_referencia = calcular_distancia_escala(puntos_referencia)\n",
    "                    escala_usuario = calcular_distancia_escala(puntos_usuario)\n",
    "\n",
    "                    similitud = calcular_similitud(puntos_referencia, puntos_usuario, escala_referencia, escala_usuario)\n",
    "\n",
    "                    if similitud < 0.1:\n",
    "                        puntuacion = 100\n",
    "                    elif similitud < 0.2:\n",
    "                        puntuacion = 50\n",
    "                    else:\n",
    "                        puntuacion = 0\n",
    "\n",
    "                    cv2.putText(frame_cam, f'Puntuacion: {puntuacion}', (10, 50),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            if results_cam.pose_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame_cam, results_cam.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "        # Mostrar ambos cuadros\n",
    "        combinado = np.hstack((cv2.resize(frame_vid, (640, 480)), cv2.resize(frame_cam, (640, 480))))\n",
    "        cv2.imshow('Comparacion: Video vs Usuario', combinado)\n",
    "\n",
    "        # Incrementar el tiempo actual según la duración de cada cuadro\n",
    "        current_time += frame_duration\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    video_cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    modo = input(\"Seleccione modo: 'extraer' para guardar puntos o 'jugar' para comparar: \")\n",
    "\n",
    "    if modo == 'extraer':\n",
    "        extraer_y_guardar_puntos(dance_video, 'puntos_referencia.json')\n",
    "    elif modo == 'jugar':\n",
    "        jugar_con_puntos('puntos_referencia.json', dance_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cap' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 180\u001b[0m\n\u001b[0;32m    178\u001b[0m     extraer_y_guardar_puntos(dance_video, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpuntos_referencia.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m modo \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjugar\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 180\u001b[0m     \u001b[43mjugar_con_puntos\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpuntos_referencia.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdance_video\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 169\u001b[0m, in \u001b[0;36mjugar_con_puntos\u001b[1;34m(json_file, video_path)\u001b[0m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mord\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mq\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    167\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 169\u001b[0m \u001b[43mcap\u001b[49m\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    170\u001b[0m video_cap\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    171\u001b[0m cv2\u001b[38;5;241m.\u001b[39mdestroyAllWindows()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cap' is not defined"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuración del video de referencia\n",
    "dance_video = 'dance_video.mp4'\n",
    "\n",
    "\n",
    "\n",
    "# Función para calcular la distancia relativa entre puntos clave\n",
    "def calcular_distancia_escala(puntos):\n",
    "    \"\"\"\n",
    "    Calcula una medida de escala basada en la distancia entre los hombros\n",
    "    (puntos 11 y 12 en Mediapipe Pose).\n",
    "    \"\"\"\n",
    "    hombro_izquierdo = puntos[11]\n",
    "    hombro_derecho = puntos[12]\n",
    "    return np.linalg.norm(np.array([hombro_izquierdo[\"x\"], hombro_izquierdo[\"y\"]]) -\n",
    "                          np.array([hombro_derecho[\"x\"], hombro_derecho[\"y\"]]))\n",
    "\n",
    "# Código para jugar comparando los puntos guardados\n",
    "\n",
    "def jugar_con_puntos(json_file, video_path):\n",
    "    # Carga los puntos del archivo JSON\n",
    "    with open(json_file, 'r') as f:\n",
    "        puntos_video_referencia = json.load(f)\n",
    "\n",
    "    #cap = cv2.VideoCapture(0)  # Cámara en vivo\n",
    "    video_cap = cv2.VideoCapture(video_path)  # Video de referencia\n",
    "\n",
    "    # def calcular_similitud(puntos1, puntos2, escala1, escala2):\n",
    "    #     \"\"\"\n",
    "    #     Calcula la similitud entre dos poses ajustando las coordenadas por sus escalas relativas.\n",
    "    #     \"\"\"\n",
    "    #     if not puntos1 or not puntos2 or len(puntos1) != len(puntos2):\n",
    "    #         return 0\n",
    "\n",
    "    #     distancias = []\n",
    "    #     for p1, p2 in zip(puntos1, puntos2):\n",
    "    #         p1_escalado = np.array([p1[\"x\"] / escala1, p1[\"y\"] / escala1, p1[\"z\"] / escala1])\n",
    "    #         p2_escalado = np.array([p2[\"x\"] / escala2, p2[\"y\"] / escala2, p2[\"z\"] / escala2])\n",
    "    #         distancias.append(np.linalg.norm(p1_escalado - p2_escalado))\n",
    "\n",
    "    #     return np.mean(distancias)\n",
    "\n",
    "    frame_idx = 0\n",
    "\n",
    "    while True:\n",
    "        # Leer cuadro del video de referencia\n",
    "        if frame_idx < len(puntos_video_referencia):\n",
    "            referencia = puntos_video_referencia[frame_idx]\n",
    "            puntos_referencia = referencia[\"puntos\"]\n",
    "            frame_index = referencia[\"frame\"]\n",
    "\n",
    "            # Posicionar el video en el frame correspondiente\n",
    "            video_cap.set(cv2.CAP_PROP_POS_FRAMES, frame_index)\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "\n",
    "            if not ret_vid:\n",
    "                break\n",
    "\n",
    "            # Dibujar puntos y conexiones en el video de referencia\n",
    "            altura, ancho, _ = frame_vid.shape\n",
    "            for conexion in mp_pose.POSE_CONNECTIONS:\n",
    "                inicio = puntos_referencia[conexion[0]]\n",
    "                fin = puntos_referencia[conexion[1]]\n",
    "                inicio_px = (int(inicio[\"x\"] * ancho), int(inicio[\"y\"] * altura))\n",
    "                fin_px = (int(fin[\"x\"] * ancho), int(fin[\"y\"] * altura))\n",
    "                cv2.line(frame_vid, inicio_px, fin_px, (255, 0, 0), 2)\n",
    "\n",
    "            for punto in puntos_referencia:\n",
    "                px = (int(punto[\"x\"] * ancho), int(punto[\"y\"] * altura))\n",
    "                cv2.circle(frame_vid, px, 5, (0, 255, 0), -1)\n",
    "\n",
    "        # Leer cuadro de la cámara en vivo\n",
    "        # ret_cam, frame_cam = cap.read()\n",
    "        # if not ret_cam:\n",
    "        #     print(\"No se puede acceder a la cámara.\")\n",
    "        #     break\n",
    "\n",
    "        # frame_cam = cv2.resize(frame_cam, None, fx=0.5, fy=0.5)\n",
    "        # rgb_frame_cam = cv2.cvtColor(frame_cam, cv2.COLOR_BGR2RGB)\n",
    "        # with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        #     results_cam = pose.process(rgb_frame_cam)\n",
    "\n",
    "        #     if results_cam.pose_landmarks:\n",
    "        #         puntos_usuario = [\n",
    "        #             {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z, \"visibility\": lm.visibility}\n",
    "        #             for lm in results_cam.pose_landmarks.landmark\n",
    "        #         ]\n",
    "\n",
    "                # if frame_idx < len(puntos_video_referencia):\n",
    "                #     escala_referencia = calcular_distancia_escala(puntos_referencia)\n",
    "                #     escala_usuario = calcular_distancia_escala(puntos_usuario)\n",
    "\n",
    "                #     similitud = calcular_similitud(puntos_referencia, puntos_usuario, escala_referencia, escala_usuario)\n",
    "\n",
    "                #     if similitud < 0.1:\n",
    "                #         puntuacion = 100\n",
    "                #     elif similitud < 0.2:\n",
    "                #         puntuacion = 50\n",
    "                #     else:\n",
    "                #         puntuacion = 0\n",
    "\n",
    "                #     cv2.putText(frame_cam, f'Puntuacion: {puntuacion}', (10, 50),\n",
    "                #                 cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "            # if results_cam.pose_landmarks:\n",
    "            #     mp_drawing.draw_landmarks(\n",
    "            #         frame_cam, results_cam.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            #         mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "            #         mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)\n",
    "            #     )\n",
    "\n",
    "        # Mostrar ambos cuadros\n",
    "        # combinado = np.hstack((cv2.resize(frame_vid, (640, 480)), cv2.resize(frame_cam, (640, 480))))\n",
    "\n",
    "        cv2.imshow('Comparacion: Video vs Usuario', frame_vid)\n",
    "\n",
    "        frame_idx += 1\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    video_cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Ejemplo de uso\n",
    "if __name__ == \"__main__\":\n",
    "    modo = input(\"Seleccione modo: 'extraer' para guardar puntos o 'jugar' para comparar: \")\n",
    "\n",
    "    if modo == 'extraer':\n",
    "        extraer_y_guardar_puntos(dance_video, 'puntos_referencia.json')\n",
    "    elif modo == 'jugar':\n",
    "        jugar_con_puntos('puntos_referencia.json', dance_video)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.11.5)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "8.837272214416531\n",
      "20.677545813386516\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pygame\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "# Inicializa los módulos de Mediapipe\n",
    "mp_pose = mp.solutions.pose\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Configuración del video de referencia\n",
    "dance_video = 'dance_video.mp4'\n",
    "\n",
    "\n",
    "# Función para calcular la distancia relativa entre puntos clave\n",
    "def calcular_distancia_escala(puntos):\n",
    "    \"\"\"\n",
    "    Calcula una medida de escala basada en la distancia entre los hombros\n",
    "    (puntos 11 y 12 en Mediapipe Pose).\n",
    "    \"\"\"\n",
    "    hombro_izquierdo = puntos[11]\n",
    "    hombro_derecho = puntos[12]\n",
    "    return np.linalg.norm(np.array([hombro_izquierdo[\"x\"], hombro_izquierdo[\"y\"]]) -\n",
    "                          np.array([hombro_derecho[\"x\"], hombro_derecho[\"y\"]]))\n",
    "\n",
    "\n",
    "def calcular_similitud(puntos1, puntos2, escala1, escala2):\n",
    "        \"\"\"\n",
    "        Calcula la similitud entre dos poses ajustando las coordenadas por sus escalas relativas.\n",
    "        \"\"\"\n",
    "        if not puntos1 or not puntos2 or len(puntos1) != len(puntos2):\n",
    "            return 0\n",
    "\n",
    "        distancias = []\n",
    "        for p1, p2 in zip(puntos1, puntos2):\n",
    "            p1_escalado = np.array([p1[\"x\"] / escala1, p1[\"y\"] / escala1, p1[\"z\"] / escala1])\n",
    "            p2_escalado = np.array([p2[\"x\"] / escala2, p2[\"y\"] / escala2, p2[\"z\"] / escala2])\n",
    "            distancias.append(np.linalg.norm(p1_escalado - p2_escalado))\n",
    "\n",
    "        return np.mean(distancias)\n",
    "\n",
    "\n",
    "\n",
    "def obtener_puntos_por_frame(json_data, frame_buscado):\n",
    "    # Buscar el frame en el JSON\n",
    "    for entrada in json_data:\n",
    "        if entrada[\"frame\"] == frame_buscado:\n",
    "            return entrada[\"puntos\"]\n",
    "    return None  # Si no se encuentra el frame\n",
    "\n",
    "\n",
    "\n",
    "def puntuacion(json_file, frame_buscado,results_cam, frame_cam):\n",
    "    puntos_referencia = obtener_puntos_por_frame (json_file, frame_buscado)\n",
    "    if(puntos_referencia):\n",
    "        puntos_usuario = [\n",
    "                    {\"x\": lm.x, \"y\": lm.y, \"z\": lm.z, \"visibility\": lm.visibility}\n",
    "                    for lm in results_cam.pose_landmarks.landmark\n",
    "                ]\n",
    "        \n",
    "        escala_referencia = calcular_distancia_escala(puntos_referencia)\n",
    "        escala_usuario = calcular_distancia_escala(puntos_usuario)\n",
    "\n",
    "        similitud = calcular_similitud(puntos_referencia, puntos_usuario, escala_referencia, escala_usuario)\n",
    "\n",
    "        print(similitud)\n",
    "    \n",
    "        if similitud < 10:\n",
    "            puntuacion = 100\n",
    "        elif similitud < 5:\n",
    "            puntuacion = 50\n",
    "        else:\n",
    "            puntuacion = 0\n",
    "\n",
    "        return(puntuacion)\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "def jugar_con_puntos(json_file, video_path):\n",
    "\n",
    "    with open(json_file, 'r') as f:\n",
    "        puntos_video_referencia = json.load(f)\n",
    "\n",
    "    # Inicializar Pygame\n",
    "    pygame.init()\n",
    "\n",
    "    # Cargar y reproducir el archivo de audio\n",
    "    audio_file = 'audio_extraido.mp3'  # Reemplaza con la ruta a tu archivo de audio\n",
    "    pygame.mixer.music.load(audio_file)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "    # Cargar el video usando OpenCV\n",
    "    cap_video = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Verifica si se pudo abrir el archivo de video\n",
    "    if not cap_video.isOpened():\n",
    "        print(\"Error: No se pudo abrir el archivo de video.\")\n",
    "        exit()\n",
    "\n",
    "    # Obtener las dimensiones del video (ancho y alto)\n",
    "    frame_width = int(cap_video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap_video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Configuración de la ventana de Pygame\n",
    "    screen_width = frame_width * 2  # Pantalla con el doble de ancho (video + cámara)\n",
    "    screen_height = frame_height  # Mantener la altura igual\n",
    "    screen = pygame.display.set_mode((screen_width, screen_height))  # Tamaño de la ventana\n",
    "\n",
    "    # Obtener la tasa de fotogramas del video (FPS)\n",
    "    video_frame_rate = cap_video.get(cv2.CAP_PROP_FPS)\n",
    "\n",
    "    # Inicialización de variables\n",
    "    last_frame_index = -1\n",
    "\n",
    "    # Configura la cámara\n",
    "    cap_camera = cv2.VideoCapture(0)\n",
    "\n",
    "    # Verifica si la cámara está disponible\n",
    "    if not cap_camera.isOpened():\n",
    "        print(\"Error: No se puede acceder a la cámara.\")\n",
    "        exit()\n",
    "\n",
    "    score=0\n",
    "\n",
    "    # Inicia el modelo de detección de pose\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "\n",
    "    \n",
    "        while cap_video.isOpened() and cap_camera.isOpened():\n",
    "            # Obtiene el tiempo actual de la música en segundos\n",
    "            current_time = pygame.mixer.music.get_pos() / 1_000  # en segundos\n",
    "            current_frame_index = int(current_time * video_frame_rate)  # Índice del fotograma correspondiente\n",
    "\n",
    "            # Si el índice del fotograma ha cambiado, leemos y mostramos el nuevo fotograma\n",
    "            if current_frame_index != last_frame_index:\n",
    "                # Rewind a la posición correcta en el video\n",
    "                cap_video.set(cv2.CAP_PROP_POS_FRAMES, current_frame_index)\n",
    "\n",
    "                # Lee el fotograma del video\n",
    "                ret_video, frame_video = cap_video.read()\n",
    "                if not ret_video:\n",
    "                    break  # Si no hay más frames, salimos del bucle\n",
    "\n",
    "                # Convertir la imagen de BGR (OpenCV) a RGB (Pygame)\n",
    "                frame_rgb = cv2.cvtColor(frame_video, cv2.COLOR_BGR2RGB)\n",
    "                current_image = pygame.image.frombuffer(frame_rgb.tobytes(), frame_rgb.shape[1::-1], \"RGB\")\n",
    "\n",
    "                last_frame_index = current_frame_index\n",
    "\n",
    "\n",
    "            # Mostrar el fotograma del video a la izquierda\n",
    "            screen.blit(current_image, (0, 0))\n",
    "\n",
    "            # Leer un fotograma de la cámara\n",
    "            ret_camera, frame_camera = cap_camera.read()\n",
    "            if not ret_camera:\n",
    "                print(\"No se pudo leer el cuadro de la cámara.\")\n",
    "                break\n",
    "\n",
    "            # Procesa la imagen para detectar poses\n",
    "            # results = pose.process(rgb_frame_camera)\n",
    "\n",
    "            # # Dibuja las anotaciones de pose en la imagen original de la cámara\n",
    "            # if results.pose_landmarks:\n",
    "            #     mp_drawing.draw_landmarks(\n",
    "            #         frame_camera, results.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "            #         mp_drawing.DrawingSpec(color=(0, 255, 0), thickness=2, circle_radius=2),\n",
    "            #         mp_drawing.DrawingSpec(color=(0, 0, 255), thickness=2, circle_radius=2)\n",
    "            #     )\n",
    "            \n",
    "\n",
    "                # # Dibujar puntos y conexiones en el video de referencia\n",
    "                # altura, ancho, _ = frame_camera.shape\n",
    "                # for conexion in mp_pose.POSE_CONNECTIONS:\n",
    "                #     inicio = puntos_referencia[conexion[0]]\n",
    "                #     fin = puntos_referencia[conexion[1]]\n",
    "                #     inicio_px = (int(inicio[\"x\"] * ancho), int(inicio[\"y\"] * altura))\n",
    "                #     fin_px = (int(fin[\"x\"] * ancho), int(fin[\"y\"] * altura))\n",
    "                #     cv2.line(frame_camera, inicio_px, fin_px, (255, 0, 0), 2)\n",
    "\n",
    "                # for punto in puntos_referencia:\n",
    "                #     px = (int(punto[\"x\"] * ancho), int(punto[\"y\"] * altura))\n",
    "                #     cv2.circle(frame_camera, px, 5, (0, 255, 0), -1)\n",
    "\n",
    "            frame_camera = cv2.flip(frame_camera, 1)\n",
    "            rgb_frame_cam = cv2.cvtColor(frame_camera, cv2.COLOR_BGR2RGB)\n",
    "            results_cam = pose.process(rgb_frame_cam)\n",
    "\n",
    "\n",
    "            if results_cam.pose_landmarks:\n",
    "                if current_frame_index % 15 == 0:\n",
    "                    score += puntuacion(puntos_video_referencia,current_frame_index,results_cam, frame_camera)\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame_camera, results_cam.pose_landmarks, mp_pose.POSE_CONNECTIONS,\n",
    "                    mp_drawing.DrawingSpec(color=(0, 255, 255), thickness=2, circle_radius=2),\n",
    "                    mp_drawing.DrawingSpec(color=(255, 0, 255), thickness=2, circle_radius=2)\n",
    "                )\n",
    "\n",
    "            cv2.putText(frame_camera, f'Puntuacion: {score}', (10, 50),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "            # Convertir la imagen de la cámara a RGB para Pygame\n",
    "            frame_camera_rgb = cv2.cvtColor(frame_camera, cv2.COLOR_BGR2RGB)\n",
    "            frame_camera_surface = pygame.image.frombuffer(frame_camera_rgb.tobytes(), frame_camera_rgb.shape[1::-1], \"RGB\")\n",
    "\n",
    "            # Mostrar la imagen de la cámara a la derecha\n",
    "            screen.blit(frame_camera_surface, (frame_width, 0))\n",
    "\n",
    "            pygame.display.flip()\n",
    "\n",
    "            # Comprobar eventos, como cerrar la ventana\n",
    "            for event in pygame.event.get():\n",
    "                if event.type == pygame.QUIT:\n",
    "                    cap_video.release()\n",
    "                    cap_camera.release()\n",
    "                    pygame.quit()\n",
    "                    exit()\n",
    "\n",
    "    # Cerrar todo después de la reproducción\n",
    "    cap_video.release()\n",
    "    cap_camera.release()\n",
    "    pygame.quit()\n",
    "\n",
    "# Función para extraer puntos de referencia y guardarlos en un archivo JSON\n",
    "def extraer_y_guardar_puntos(video_path, output_file):\n",
    "    puntos_video_referencia = []\n",
    "\n",
    "    with mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5) as pose:\n",
    "        video_cap = cv2.VideoCapture(video_path)\n",
    "        while True:\n",
    "            ret_vid, frame_vid = video_cap.read()\n",
    "            if not ret_vid:\n",
    "                break\n",
    "\n",
    "            # Obtén el índice del frame actual\n",
    "            frame_index = int(video_cap.get(cv2.CAP_PROP_POS_FRAMES))\n",
    "\n",
    "            rgb_frame_vid = cv2.cvtColor(frame_vid, cv2.COLOR_BGR2RGB)\n",
    "            results_vid = pose.process(rgb_frame_vid)\n",
    "\n",
    "            if results_vid.pose_landmarks:\n",
    "                puntos = []\n",
    "                for landmark in results_vid.pose_landmarks.landmark:\n",
    "                    puntos.append({\n",
    "                        \"x\": landmark.x,\n",
    "                        \"y\": landmark.y,\n",
    "                        \"z\": landmark.z,\n",
    "                        \"visibility\": landmark.visibility\n",
    "                    })\n",
    "\n",
    "                # Agrega los puntos junto con el índice del frame al resultado\n",
    "                puntos_video_referencia.append({\n",
    "                    \"frame\": frame_index,\n",
    "                    \"puntos\": puntos\n",
    "                })\n",
    "\n",
    "        video_cap.release()\n",
    "\n",
    "    # Guarda los puntos en un archivo JSON\n",
    "    with open(output_file, 'w') as f:\n",
    "        json.dump(puntos_video_referencia, f)\n",
    "\n",
    "    print(f\"Puntos de referencia guardados en {output_file}\")\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    modo = input(\"Seleccione modo: 'extraer' para guardar puntos o 'jugar' para comparar: \")\n",
    "\n",
    "    if modo == 'extraer':\n",
    "        extraer_y_guardar_puntos(dance_video, 'puntos_referencia.json')\n",
    "    elif modo == 'jugar':\n",
    "        jugar_con_puntos('puntos_referencia.json', dance_video)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "VC_P5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
