{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copyright 2020-2021 The MediaPipe Authors.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#      http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\n",
    "\"\"\"MediaPipe Pose.\"\"\"\n",
    "\n",
    "import enum\n",
    "from typing import NamedTuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# The following imports are needed because python pb2 silently discards\n",
    "# unknown protobuf fields.\n",
    "# pylint: disable=unused-import\n",
    "from mediapipe.calculators.core import constant_side_packet_calculator_pb2\n",
    "from mediapipe.calculators.core import gate_calculator_pb2\n",
    "from mediapipe.calculators.core import split_vector_calculator_pb2\n",
    "from mediapipe.calculators.image import warp_affine_calculator_pb2\n",
    "from mediapipe.calculators.tensor import image_to_tensor_calculator_pb2\n",
    "from mediapipe.calculators.tensor import inference_calculator_pb2\n",
    "from mediapipe.calculators.tensor import tensors_to_classification_calculator_pb2\n",
    "from mediapipe.calculators.tensor import tensors_to_detections_calculator_pb2\n",
    "from mediapipe.calculators.tensor import tensors_to_landmarks_calculator_pb2\n",
    "from mediapipe.calculators.tensor import tensors_to_segmentation_calculator_pb2\n",
    "from mediapipe.calculators.tflite import ssd_anchors_calculator_pb2\n",
    "from mediapipe.calculators.util import detections_to_rects_calculator_pb2\n",
    "from mediapipe.calculators.util import landmarks_smoothing_calculator_pb2\n",
    "from mediapipe.calculators.util import local_file_contents_calculator_pb2\n",
    "from mediapipe.calculators.util import logic_calculator_pb2\n",
    "from mediapipe.calculators.util import non_max_suppression_calculator_pb2\n",
    "from mediapipe.calculators.util import rect_transformation_calculator_pb2\n",
    "from mediapipe.calculators.util import thresholding_calculator_pb2\n",
    "from mediapipe.calculators.util import visibility_smoothing_calculator_pb2\n",
    "from mediapipe.framework.tool import switch_container_pb2\n",
    "# pylint: enable=unused-import\n",
    "from mediapipe.python.solution_base import SolutionBase\n",
    "from mediapipe.python.solutions import download_utils\n",
    "# pylint: disable=unused-import\n",
    "from mediapipe.python.solutions.pose_connections import POSE_CONNECTIONS\n",
    "# pylint: enable=unused-import\n",
    "\n",
    "\n",
    "class PoseLandmark(enum.IntEnum):\n",
    "  \"\"\"The 33 pose landmarks.\"\"\"\n",
    "  NOSE = 0\n",
    "  LEFT_EYE_INNER = 1\n",
    "  LEFT_EYE = 2\n",
    "  LEFT_EYE_OUTER = 3\n",
    "  RIGHT_EYE_INNER = 4\n",
    "  RIGHT_EYE = 5\n",
    "  RIGHT_EYE_OUTER = 6\n",
    "  LEFT_EAR = 7\n",
    "  RIGHT_EAR = 8\n",
    "  MOUTH_LEFT = 9\n",
    "  MOUTH_RIGHT = 10\n",
    "  LEFT_SHOULDER = 11\n",
    "  RIGHT_SHOULDER = 12\n",
    "  LEFT_ELBOW = 13\n",
    "  RIGHT_ELBOW = 14\n",
    "  LEFT_WRIST = 15\n",
    "  RIGHT_WRIST = 16\n",
    "  LEFT_PINKY = 17\n",
    "  RIGHT_PINKY = 18\n",
    "  LEFT_INDEX = 19\n",
    "  RIGHT_INDEX = 20\n",
    "  LEFT_THUMB = 21\n",
    "  RIGHT_THUMB = 22\n",
    "  LEFT_HIP = 23\n",
    "  RIGHT_HIP = 24\n",
    "  LEFT_KNEE = 25\n",
    "  RIGHT_KNEE = 26\n",
    "  LEFT_ANKLE = 27\n",
    "  RIGHT_ANKLE = 28\n",
    "  LEFT_HEEL = 29\n",
    "  RIGHT_HEEL = 30\n",
    "  LEFT_FOOT_INDEX = 31\n",
    "  RIGHT_FOOT_INDEX = 32\n",
    "\n",
    "\n",
    "_BINARYPB_FILE_PATH = 'mediapipe/modules/pose_landmark/pose_landmark_cpu.binarypb'\n",
    "\n",
    "\n",
    "def _download_oss_pose_landmark_model(model_complexity):\n",
    "  \"\"\"Downloads the pose landmark lite/heavy model from the MediaPipe Github repo if it doesn't exist in the package.\"\"\"\n",
    "\n",
    "  if model_complexity == 0:\n",
    "    download_utils.download_oss_model(\n",
    "        'mediapipe/modules/pose_landmark/pose_landmark_lite.tflite')\n",
    "  elif model_complexity == 2:\n",
    "    download_utils.download_oss_model(\n",
    "        'mediapipe/modules/pose_landmark/pose_landmark_heavy.tflite')\n",
    "\n",
    "\n",
    "class Pose(SolutionBase):\n",
    "  \"\"\"MediaPipe Pose.\n",
    "\n",
    "  MediaPipe Pose processes an RGB image and returns pose landmarks on the most\n",
    "  prominent person detected.\n",
    "\n",
    "  Please refer to https://solutions.mediapipe.dev/pose#python-solution-api for\n",
    "  usage examples.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,\n",
    "               static_image_mode=False,\n",
    "               model_complexity=1,\n",
    "               smooth_landmarks=True,\n",
    "               enable_segmentation=False,\n",
    "               smooth_segmentation=True,\n",
    "               min_detection_confidence=0.5,\n",
    "               min_tracking_confidence=0.5):\n",
    "    \"\"\"Initializes a MediaPipe Pose object.\n",
    "\n",
    "    Args:\n",
    "      static_image_mode: Whether to treat the input images as a batch of static\n",
    "        and possibly unrelated images, or a video stream. See details in\n",
    "        https://solutions.mediapipe.dev/pose#static_image_mode.\n",
    "      model_complexity: Complexity of the pose landmark model: 0, 1 or 2. See\n",
    "        details in https://solutions.mediapipe.dev/pose#model_complexity.\n",
    "      smooth_landmarks: Whether to filter landmarks across different input\n",
    "        images to reduce jitter. See details in\n",
    "        https://solutions.mediapipe.dev/pose#smooth_landmarks.\n",
    "      enable_segmentation: Whether to predict segmentation mask. See details in\n",
    "        https://solutions.mediapipe.dev/pose#enable_segmentation.\n",
    "      smooth_segmentation: Whether to filter segmentation across different input\n",
    "        images to reduce jitter. See details in\n",
    "        https://solutions.mediapipe.dev/pose#smooth_segmentation.\n",
    "      min_detection_confidence: Minimum confidence value ([0.0, 1.0]) for person\n",
    "        detection to be considered successful. See details in\n",
    "        https://solutions.mediapipe.dev/pose#min_detection_confidence.\n",
    "      min_tracking_confidence: Minimum confidence value ([0.0, 1.0]) for the\n",
    "        pose landmarks to be considered tracked successfully. See details in\n",
    "        https://solutions.mediapipe.dev/pose#min_tracking_confidence.\n",
    "    \"\"\"\n",
    "    _download_oss_pose_landmark_model(model_complexity)\n",
    "    super().__init__(\n",
    "        binary_graph_path=_BINARYPB_FILE_PATH,\n",
    "        side_inputs={\n",
    "            'model_complexity': model_complexity,\n",
    "            'smooth_landmarks': smooth_landmarks and not static_image_mode,\n",
    "            'enable_segmentation': enable_segmentation,\n",
    "            'smooth_segmentation':\n",
    "                smooth_segmentation and not static_image_mode,\n",
    "            'use_prev_landmarks': not static_image_mode,\n",
    "        },\n",
    "        calculator_params={\n",
    "            'posedetectioncpu__TensorsToDetectionsCalculator.min_score_thresh':\n",
    "                min_detection_confidence,\n",
    "            'poselandmarkbyroicpu__tensorstoposelandmarksandsegmentation__ThresholdingCalculator.threshold':\n",
    "                min_tracking_confidence,\n",
    "        },\n",
    "        outputs=['pose_landmarks', 'pose_world_landmarks', 'segmentation_mask'])\n",
    "\n",
    "  def process(self, image: np.ndarray) -> NamedTuple:\n",
    "    \"\"\"Processes an RGB image and returns the pose landmarks on the most prominent person detected.\n",
    "\n",
    "    Args:\n",
    "      image: An RGB image represented as a numpy ndarray.\n",
    "\n",
    "    Raises:\n",
    "      RuntimeError: If the underlying graph throws any error.\n",
    "      ValueError: If the input image is not three channel RGB.\n",
    "\n",
    "    Returns:\n",
    "      A NamedTuple with fields describing the landmarks on the most prominate\n",
    "      person detected:\n",
    "        1) \"pose_landmarks\" field that contains the pose landmarks.\n",
    "        2) \"pose_world_landmarks\" field that contains the pose landmarks in\n",
    "        real-world 3D coordinates that are in meters with the origin at the\n",
    "        center between hips.\n",
    "        3) \"segmentation_mask\" field that contains the segmentation mask if\n",
    "           \"enable_segmentation\" is set to true.\n",
    "    \"\"\"\n",
    "\n",
    "    results = super().process(input_data={'image': image})\n",
    "    if results.pose_landmarks:  # pytype: disable=attribute-error\n",
    "      for landmark in results.pose_landmarks.landmark:  # pytype: disable=attribute-error\n",
    "        landmark.ClearField('presence')\n",
    "    if results.pose_world_landmarks:  # pytype: disable=attribute-error\n",
    "      for landmark in results.pose_world_landmarks.landmark:  # pytype: disable=attribute-error\n",
    "        landmark.ClearField('presence')\n",
    "    return results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
